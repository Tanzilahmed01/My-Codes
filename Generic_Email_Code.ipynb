{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxzRBTx43RachPrDp50kHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanzilahmed01/My-Codes/blob/main/Generic_Email_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGHlTu-mTvnA"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# üìå Colab Ready Script (Manual Domain Input)\n",
        "# ==========================\n",
        "import pandas as pd\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import re\n",
        "from urllib.parse import quote\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files, output\n",
        "import nest_asyncio\n",
        "\n",
        "# ===== CONFIG =====\n",
        "OUTPUT_FILE = \"domains_with_emails.csv\"\n",
        "MAX_CONCURRENT = 5   # concurrency limit\n",
        "MAX_SEARCH_RESULTS = 5  # DuckDuckGo pages to crawl\n",
        "\n",
        "# Regex for emails\n",
        "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "\n",
        "# Generic email prefixes\n",
        "GENERIC_PREFIXES = [\n",
        "    \"info\",\"contact\",\"support\",\"help\",\"hello\",\"hi\",\"admin\",\"office\",\"team\",\n",
        "    \"sales\",\"marketing\",\"business\",\"enquiry\",\"enquiries\",\"service\",\"services\",\n",
        "    \"mail\",\"email\",\"customerservice\",\"customer.service\",\"customersupport\",\n",
        "    \"customer.support\",\"clientcare\",\"client.services\",\"order\",\"orders\",\n",
        "    \"booking\",\"bookings\",\"reservation\",\"reservations\",\"billing\",\"accounts\",\n",
        "    \"accounting\",\"finance\",\"payment\",\"payments\",\"invoice\",\"invoices\",\"hr\",\n",
        "    \"jobs\",\"career\",\"careers\",\"work\",\"recruitment\",\"talent\",\"press\",\"media\",\n",
        "    \"pr\",\"news\",\"newsletter\",\"subscribe\",\"unsubscribe\",\"legal\",\"compliance\",\n",
        "    \"privacy\",\"security\",\"noreply\",\"no-reply\",\"donotreply\",\"do-not-reply\",\n",
        "    \"postmaster\",\"webmaster\",\"hostmaster\",\"abuse\",\"us\",\"fan\",\"sales.austria\",\n",
        "    \"helpdesk\",\"supportteam\",\"techsupport\",\"customersuccess\",\"servicedesk\",\"feedback\",\n",
        "    \"operations\",\"adminteam\",\"officeadmin\",\"management\",\"hrteam\",\"finance.team\",\"accounting.team\",\n",
        "    \"procurement\",\"logistics\",\"contactus\",\"info.team\",\"inquiry\",\"communication\",\"connect\",\"teamcontact\",\n",
        "    \"notifications\",\"updates\",\"alerts\",\"system\",\"automated\",\"robot\",\"founder\",\"ceo\",\"coo\",\"cfo\",\n",
        "    \"admin.office\",\"partners\",\"clients\",\"manager\",\"staff\",\"teamlead\",\"support.office\",\"help.office\",\n",
        "    \"office.support\",\"service.team\",\"client.support\",\"customer.success\",\"business.team\",\"team.services\",\n",
        "    \"team.office\",\"office.team\",\"supportdesk\",\"client.services\",\"client.team\",\"office.contact\",\"team.contact\",\n",
        "    \"customer.care\",\"client.care\",\"office.admins\",\"team.admins\",\"support.center\",\"help.center\",\"info.center\",\n",
        "    \"queries\",\"ask\",\"reachus\",\"care\",\"clientcare\",\"customercare\",\"assistance\",\"complaints\",\"resolve\",\n",
        "    \"bizdev\",\"partnerships\",\"promotions\",\"outreach\",\"offers\",\"deals\",\"growth\",\n",
        "    \"data\",\"propertydata\",\"realestatedata\",\"research\",\"records\",\"reports\",\"listings\",\"assets\",\"valuations\",\"analytics\",\n",
        "    \"payroll\",\"terms\",\"contracts\",\"notary\",\"registry\",\"ownership\",\"title\",\"claims\",\n",
        "    \"usa\",\"uk\",\"eu\",\"apac\",\"global\",\"local\",\"regional\",\"national\",\"international\",\"hq\",\n",
        "    \"properties\",\"estates\",\"housing\",\"rentals\",\"leasing\",\"buyers\",\"sellers\",\"tenants\",\"landlords\",\"investors\",\n",
        "    \"projects\",\"developments\",\"construction\",\"planning\",\"zoning\",\"permits\",\"approvals\",\"architecture\",\"engineering\",\"design\",\n",
        "    \"post\",\"reply\",\"relations\"\n",
        "]\n",
        "\n",
        "PRIORITY_PREFIXES = [\"info\", \"contact\", \"support\"]\n",
        "\n",
        "def is_generic(email: str) -> bool:\n",
        "    return any(email.lower().startswith(prefix + \"@\") for prefix in GENERIC_PREFIXES)\n",
        "\n",
        "def choose_best_email(emails: set) -> str:\n",
        "    \"\"\"Choose the best email based on priority prefixes, fallback alphabetical\"\"\"\n",
        "    emails = sorted(emails)\n",
        "    for prefix in PRIORITY_PREFIXES:\n",
        "        for e in emails:\n",
        "            if e.lower().startswith(prefix + \"@\"):\n",
        "                return e\n",
        "    return emails[0] if emails else None\n",
        "\n",
        "async def fetch(session, url):\n",
        "    try:\n",
        "        async with session.get(url, timeout=12) as resp:\n",
        "            if resp.status == 200:\n",
        "                return await resp.text()\n",
        "    except:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "async def scrape_website_for_email(session, domain: str):\n",
        "    urls_to_try = [\n",
        "        f\"http://{domain}\",\n",
        "        f\"https://{domain}\",\n",
        "        f\"http://{domain}/contact\",\n",
        "        f\"https://{domain}/contact\",\n",
        "        f\"http://{domain}/about\",\n",
        "        f\"https://{domain}/about\",\n",
        "        f\"http://{domain}/privacy\",\n",
        "        f\"https://{domain}/privacy\",\n",
        "    ]\n",
        "    found = set()\n",
        "    for url in urls_to_try:\n",
        "        html = await fetch(session, url)\n",
        "        if html:\n",
        "            emails = re.findall(EMAIL_REGEX, html)\n",
        "            for e in emails:\n",
        "                if e.lower().endswith(\"@\" + domain.lower()) and is_generic(e):\n",
        "                    found.add(e)\n",
        "    return list(found)\n",
        "\n",
        "async def scrape_skymem(session, domain: str):\n",
        "    url = f\"http://www.skymem.info/srch?q={quote(domain)}\"\n",
        "    found = set()\n",
        "    html = await fetch(session, url)\n",
        "    if html:\n",
        "        emails = re.findall(EMAIL_REGEX, html)\n",
        "        for e in emails:\n",
        "            if e.lower().endswith(\"@\" + domain.lower()) and is_generic(e):\n",
        "                found.add(e)\n",
        "    return list(found)\n",
        "\n",
        "async def scrape_duckduckgo(session, domain: str):\n",
        "    queries = [\"contact\", \"support\", \"info\", \"team\", \"email\"]\n",
        "    found = set()\n",
        "    for q in queries:\n",
        "        search_url = f\"https://html.duckduckgo.com/html/?q={quote(domain + ' ' + q + ' email')}\"\n",
        "        html = await fetch(session, search_url)\n",
        "        if not html:\n",
        "            continue\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        links = [a[\"href\"] for a in soup.select(\"a.result__a\") if a.get(\"href\")]\n",
        "        links = links[:MAX_SEARCH_RESULTS]\n",
        "        for link in links:\n",
        "            page_html = await fetch(session, link)\n",
        "            if page_html:\n",
        "                emails = re.findall(EMAIL_REGEX, page_html)\n",
        "                for e in emails:\n",
        "                    if e.lower().endswith(\"@\" + domain.lower()) and is_generic(e):\n",
        "                        found.add(e)\n",
        "    return list(found)\n",
        "\n",
        "async def process_domain(session, sem, domain: str):\n",
        "    async with sem:\n",
        "        print(f\"üîé Searching for {domain}...\")\n",
        "        results = set()\n",
        "        results.update(await scrape_website_for_email(session, domain))\n",
        "        results.update(await scrape_skymem(session, domain))\n",
        "        results.update(await scrape_duckduckgo(session, domain))\n",
        "        if results:\n",
        "            chosen_email = choose_best_email(results)\n",
        "            print(f\"‚úÖ {domain} -> {chosen_email}\")\n",
        "            return chosen_email\n",
        "        else:\n",
        "            print(f\"‚ùå {domain} -> Not found\")\n",
        "            return \"Not found\"\n",
        "\n",
        "async def main(domains):\n",
        "    sem = asyncio.Semaphore(MAX_CONCURRENT)\n",
        "    async with aiohttp.ClientSession(headers={\"User-Agent\": \"Mozilla/5.0\"}) as session:\n",
        "        tasks = [process_domain(session, sem, domain.strip()) for domain in domains if domain.strip()]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "    df = pd.DataFrame({\"Domain\": domains, \"Generic_Email\": results})\n",
        "    df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"üéâ Done! Results saved in\", OUTPUT_FILE)\n",
        "    files.download(OUTPUT_FILE)\n",
        "\n",
        "# ==========================\n",
        "# üöÄ Run in Colab\n",
        "# ==========================\n",
        "print(\"üìã Please paste your domains below (one per line) and press Enter (Shift+Enter to run):\")\n",
        "\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "textarea = widgets.Textarea(\n",
        "    placeholder=\"example.com\\ntestsite.org\\nmycompany.co.uk\",\n",
        "    description=\"Domains:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"200px\"),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "display(textarea)\n",
        "\n",
        "button = widgets.Button(description=\"Start Finding Emails üöÄ\", button_style='success')\n",
        "output_box = widgets.Output()\n",
        "display(button, output_box)\n",
        "\n",
        "def on_button_click(b):\n",
        "    with output_box:\n",
        "        output_box.clear_output()\n",
        "        domain_text = textarea.value.strip()\n",
        "        if not domain_text:\n",
        "            print(\"‚ö†Ô∏è Please paste at least one domain.\")\n",
        "            return\n",
        "        domains = [d.strip() for d in domain_text.split(\"\\n\") if d.strip()]\n",
        "        nest_asyncio.apply()\n",
        "        asyncio.run(main(domains))\n",
        "\n",
        "button.on_click(on_button_click)"
      ]
    }
  ]
}