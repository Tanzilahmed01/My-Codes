{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgzpSaUdyS9keK5UdGXeEc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanzilahmed01/My-Codes/blob/main/Linkedin_Scraper2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ðŸ“Œ LinkedIn Company Scraper (Auto Pause & Resume on Network Loss)\n",
        "# ==========================\n",
        "!pip install requests beautifulsoup4 pandas lxml\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import files\n",
        "import re\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/128.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "BAD_DOMAINS = [\n",
        "    \"linkedin.com\", \"google.com\", \"bing.com\", \"facebook.com\",\n",
        "    \"instagram.com\", \"twitter.com\", \"x.com\", \"youtube.com\",\n",
        "    \"maps\", \"goo.gl\", \"bit.ly\", \"tinyurl.com\"\n",
        "]\n",
        "\n",
        "VALID_DOMAINS = [\".com\", \".org\", \".net\", \".io\", \".co\", \".in\", \".biz\", \".ai\", \".tech\"]\n",
        "\n",
        "# === NETWORK CHECK ===\n",
        "def check_connection(url=\"https://www.google.com\", timeout=5):\n",
        "    try:\n",
        "        requests.get(url, timeout=timeout)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def wait_for_connection():\n",
        "    print(\"\\nðŸš¨ Network lost! Waiting to reconnect...\")\n",
        "    while not check_connection():\n",
        "        print(\"â³ Still waiting for network...\", end=\"\\r\")\n",
        "        time.sleep(5)\n",
        "    print(\"\\nâœ… Network reconnected! Resuming scraping...\\n\")\n",
        "\n",
        "# === URL CLEANER ===\n",
        "def clean_url(url):\n",
        "    url = url.strip()\n",
        "    url = re.sub(r\"^https://www\\.linkedin\\.com/safety/go\\?url=\", \"\", url)\n",
        "    url = re.sub(r\"%3A\", \":\", url)\n",
        "    url = re.sub(r\"%2F\", \"/\", url)\n",
        "    url = re.sub(r\"%3F\", \"?\", url)\n",
        "    url = re.sub(r\"%3D\", \"=\", url)\n",
        "    url = re.sub(r\"%26\", \"&\", url)\n",
        "    url = url.split(\"?\")[0]\n",
        "    return url\n",
        "\n",
        "# === EMPLOYEE COUNT ===\n",
        "def extract_employee_count(soup):\n",
        "    employee_count = None\n",
        "    full_text = soup.get_text()\n",
        "    patterns = [\n",
        "        r'(\\d{1,3}(?:,\\d{3})(?:-\\d{1,3}(?:,\\d{3}))?|\\d{1,3}(?:,\\d{3})\\+\\s)employees?\\b',\n",
        "        r'(\\d{1,3}(?:,\\d{3})(?:,\\d{3}))\\semployees?\\son\\s*LinkedIn\\b'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, full_text, re.I)\n",
        "        if match:\n",
        "            employee_count = re.sub(r'\\s+', ' ', match.group(1).strip())\n",
        "            break\n",
        "    return employee_count\n",
        "\n",
        "# === PRIMARY ADDRESS ===\n",
        "def extract_primary_address(soup):\n",
        "    primary_address = None\n",
        "    full_text = soup.get_text()\n",
        "    patterns = [\n",
        "        r'Headquarters?[:\\s]*([^\\n\\r]+?)(?=\\n|$)',\n",
        "        r'Location[:\\s]*([^\\n\\r]+?)(?=\\n|$)',\n",
        "        r'Based in[:\\s]*([^\\n\\r]+?)(?=\\n|$)'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, full_text, re.I)\n",
        "        if match:\n",
        "            primary_address = re.sub(r'\\s+', ' ', match.group(1).strip())\n",
        "            if len(primary_address) > 100:\n",
        "                primary_address = primary_address[:100] + '...'\n",
        "            break\n",
        "    return primary_address\n",
        "\n",
        "# === WEBSITE ===\n",
        "def extract_clean_website(soup):\n",
        "    website = None\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = clean_url(a[\"href\"])\n",
        "        text = (a.get_text() or \"\").lower()\n",
        "        if (\n",
        "            href.startswith(\"http\")\n",
        "            and any(ext in href for ext in VALID_DOMAINS)\n",
        "            and not any(bad in href for bad in BAD_DOMAINS)\n",
        "            and (\"website\" in text or \"visit\" in text or \"official\" in text)\n",
        "        ):\n",
        "            return href\n",
        "    return website\n",
        "\n",
        "# === MAIN SCRAPER ===\n",
        "def scrape_company(url):\n",
        "    data = {\"url\": url}\n",
        "    while not check_connection():\n",
        "        wait_for_connection()\n",
        "    try:\n",
        "        res = requests.get(url, headers=HEADERS, timeout=15)\n",
        "        res.raise_for_status()\n",
        "    except Exception as e:\n",
        "        return {\"url\": url, \"error\": str(e)}\n",
        "    soup = BeautifulSoup(res.text, \"lxml\")\n",
        "    data[\"name\"] = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else None\n",
        "    data[\"tagline\"] = soup.find(\"h2\").get_text(strip=True) if soup.find(\"h2\") else None\n",
        "    data[\"about\"] = soup.find(\"p\").get_text(strip=True) if soup.find(\"p\") else None\n",
        "    data[\"employees\"] = extract_employee_count(soup)\n",
        "    data[\"primary_address\"] = extract_primary_address(soup)\n",
        "    data[\"website\"] = extract_clean_website(soup)\n",
        "    return data\n",
        "\n",
        "# === MANUAL INPUT (COLAB) ===\n",
        "print(\"ðŸ“‹ Paste LinkedIn company URLs below (one per line) then click 'Start Scraping ðŸš€'\")\n",
        "\n",
        "textarea = widgets.Textarea(\n",
        "    placeholder=\"https://www.linkedin.com/company/example/\\nhttps://www.linkedin.com/company/testinc/\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"200px\"),\n",
        "    description=\"Company URLs:\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "display(textarea)\n",
        "\n",
        "button = widgets.Button(description=\"Start Scraping ðŸš€\", button_style='success')\n",
        "output_box = widgets.Output()\n",
        "display(button, output_box)\n",
        "\n",
        "def on_button_click(b):\n",
        "    with output_box:\n",
        "        output_box.clear_output()\n",
        "        url_text = textarea.value.strip()\n",
        "        if not url_text:\n",
        "            print(\"âš ï¸ Please paste at least one LinkedIn company URL.\")\n",
        "            return\n",
        "        urls = [u.strip() for u in url_text.split(\"\\n\") if u.strip()]\n",
        "        results = []\n",
        "        checkpoint_file = \"linkedin_scraper_checkpoint.csv\"\n",
        "\n",
        "        # Resume if checkpoint exists\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            print(\"ðŸŸ¡ Found previous progress â€” resuming from checkpoint.\")\n",
        "            old_df = pd.read_csv(checkpoint_file)\n",
        "            done_urls = old_df[\"url\"].tolist()\n",
        "            results = old_df.to_dict(\"records\")\n",
        "            urls = [u for u in urls if u not in done_urls]\n",
        "        else:\n",
        "            done_urls = []\n",
        "\n",
        "        print(f\"ðŸ” Starting scraping for {len(urls)} new companies...\")\n",
        "\n",
        "        for i, url in enumerate(urls, 1):\n",
        "            print(f\"\\n[{i}/{len(urls)}] Scraping: {url}\")\n",
        "            data = scrape_company(url)\n",
        "            results.append(data)\n",
        "            if \"error\" in data:\n",
        "                print(f\"  âŒ ERROR: {data['error']}\")\n",
        "            else:\n",
        "                print(\"  âœ… EXTRACTED DATA:\")\n",
        "                print(f\"     Name: {data.get('name', 'N/A')}\")\n",
        "                print(f\"     Tagline: {data.get('tagline', 'N/A')}\")\n",
        "                about_text = data.get('about') or ''\n",
        "                truncated_about = about_text[:100] + ('...' if len(about_text) > 100 else '')\n",
        "                print(f\"     About: {truncated_about}\")\n",
        "                print(f\"     Employees: {data.get('employees', 'N/A')}\")\n",
        "                print(f\"     Primary Address: {data.get('primary_address', 'N/A')}\")\n",
        "                print(f\"     Website: {data.get('website', 'N/A')}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "            # Save checkpoint every 5 results\n",
        "            if len(results) % 25 == 0:\n",
        "                pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
        "                print(\"ðŸ’¾ Progress saved to checkpoint.\")\n",
        "\n",
        "            time.sleep(2)\n",
        "\n",
        "        df_output = pd.DataFrame(results)\n",
        "        output_name = \"linkedin_companies_results_manual.csv\"\n",
        "        df_output.to_csv(output_name, index=False)\n",
        "        print(f\"\\nâœ… Done! Results saved to '{output_name}'\")\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            os.remove(checkpoint_file)\n",
        "        files.download(output_name)\n",
        "\n",
        "button.on_click(on_button_click)\n"
      ],
      "metadata": {
        "id": "c-qBRY7iZfpz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}